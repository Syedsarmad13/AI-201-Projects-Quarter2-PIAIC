{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Project 1: LangChain Hello World Project\n",
        "\n",
        "This project involves building a LangChain notebook in Google Colab that utilizes the Google Gemini Flash 2.0 model to respond to user questions. The provided example serves as a starting point, assuming access to the Gemini API and a basic Python setup. However, the final project must be implemented and submitted through Google Colab."
      ],
      "metadata": {
        "id": "LSsvaNXLM-z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Required Libraries\n",
        "Run the following commands to ensure all required libraries are installed:"
      ],
      "metadata": {
        "id": "UR3xrzDgAxZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install python-dotenv  # Optional, for managing API keys\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhEf-DQW6iSu",
        "outputId": "2c0ade42-294d-4644-ecd8-311e02ac83af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Libraries\n",
        "Update your imports to include the correct modules for Google Gemini:"
      ],
      "metadata": {
        "id": "PpF72Vy0AtxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any\n"
      ],
      "metadata": {
        "id": "5j5Ry0sD7Nm8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Set Up the Gemini API\n",
        "Obtain your API key from the Google AI Studio or Google Cloud, then configure the Gemini model:\n"
      ],
      "metadata": {
        "id": "6ARplUYIAo7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gnfgcczT8Zqd",
        "outputId": "6b183850-e019-480f-a092-f591af62d8dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyC0fOcHf4nsKyhjM6v0OTwhH04rJhPXdGw'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Retrieve the API key securely\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Verify if the key is loaded\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in userdata.\")\n",
        "\n",
        "# Initialize the Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x1wLCszo7jEG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create a Custom Wrapper for Google Gemini\n",
        "LangChain allows creating a custom LLM class. Here’s the wrapper for Gemini:"
      ],
      "metadata": {
        "id": "TIeJKOejAdbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model: str = \"gemini-pro\"  # Model name\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}\n"
      ],
      "metadata": {
        "id": "W-upmihz70p9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Integrate with LangChain\n",
        "Now you can use the GeminiLLM class as a replacement for GoogleGeminiFlash:"
      ],
      "metadata": {
        "id": "3X95g6koAXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini LLM\n",
        "llm = GeminiLLM(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XABPrb5X73Q1",
        "outputId": "c2cf52d4-8f7b-45fb-c66f-781e3ffcec33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cd95223d9b75>:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Ask a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "n9RBpuON8kE-",
        "outputId": "503e8689-95f6-49ed-bbb7-74890f026af1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a3996980d26a>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a decentralized network for machine translation. It is a blockchain-based platform that allows users to contribute their own translations to a shared database. This database is then used to train machine translation models that can be used to translate text between any two languages. LangChain is designed to be a more efficient and cost-effective way to train machine translation models than traditional methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "4cpqPPxOFAuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Steps in Colab Project**"
      ],
      "metadata": {
        "id": "P9PForA_E71X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Experiment with Prompts\n",
        "You can create multiple prompt templates to see how the model responds to various formats and tasks."
      ],
      "metadata": {
        "id": "OPK-xaQgBgu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for answering factual questions\n",
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Template for creative storytelling\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a creative storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "# Using a different template\n",
        "question = \"Tell me a short story about a quaid e azam.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "3iXQJD1M_96r",
        "outputId": "20a97871-fa8c-4941-a8de-d6bd0b85fe1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: In the heart of Bombay, amidst the bustling crowds, lived a young lawyer named Muhammad Ali Jinnah. With his sharp intellect and unwavering determination, Jinnah emerged as a beacon of hope for the Muslim community.\n",
            "\n",
            "One fateful day, as Jinnah walked through the city streets, he witnessed the unjust treatment of Muslims by the British Raj. A fire ignited within him, a fire that would forever alter the course of history.\n",
            "\n",
            "Inspired by his convictions, Jinnah tirelessly advocated for the rights of Muslims. He eloquently argued for separate electorates, a system that would ensure their political representation. His speeches, filled with passion and logic, resonated with the masses.\n",
            "\n",
            "As the movement for Muslim self-determination gained momentum, Jinnah became known as Quaid-e-Azam, the Great Leader. He rallied Muslims across the subcontinent, uniting them under the banner of the All-India Muslim League.\n",
            "\n",
            "Through unwavering resolve and diplomatic maneuvering, Quaid-e-Azam negotiated with the British government and secured the creation of a separate Muslim state. On August 14, 1947, Pakistan emerged as an independent nation, a testament to the indomitable spirit of its founding father.\n",
            "\n",
            "As the first Governor-General of Pakistan, Quaid-e-Azam tirelessly worked to build a prosperous and progressive nation. He laid the foundations of a democratic society, emphasizing the principles of unity, equality, and tolerance.\n",
            "\n",
            "However, his tenure was cut short by a tragic illness. On September 11, 1948, Quaid-e-Azam passed away, leaving behind a legacy that would forever inspire generations to come.\n",
            "\n",
            "Today, Quaid-e-Azam's name is etched in the annals of history as a visionary leader who fought relentlessly for the rights of his people. His unwavering belief in the power of self-determination and his unwavering commitment to justice continue to guide the destiny of Pakistan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Add Memory (Multi-Turn Conversations)\n",
        "LangChain provides memory for maintaining context across multiple interactions."
      ],
      "metadata": {
        "id": "CHce1kU_Cmjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reuire module installation\n",
        "!pip install langchain-experimental -q\n"
      ],
      "metadata": {
        "id": "zbUhavVOBTOh",
        "outputId": "f0fb1a4a-2d4e-4980-e402-babf7347613c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Start a conversation\n",
        "response1 = conversation.run(\"What is LangChain?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple words?\")\n",
        "print(\"Q2:\", response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "aeCceKIjC2yP",
        "outputId": "48f06aed-3c65-48f1-9744-35018435cd03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0daba4cfee94>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-10-0daba4cfee94>:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: LangChain is a massively multilingual dataset of 100+ languages, with 230+ parallel corpora and 1.3B+ sentence pairs. It is designed to support research in multilingual natural language processing (NLP).\n",
            "Q2: LangChain is like a giant collection of texts in over 100 different languages. It's like a library, but instead of books, it has sentences and paragraphs in different languages. Researchers use LangChain to study how computers can understand and translate languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMUSpBeEDFMG",
        "outputId": "559ab625-e85d-4382-d17b-7bf7e6c0115a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is a massively multilingual dataset of 100+ languages, with 230+ parallel corpora and 1.3B+ sentence pairs. It is designed to support research in multilingual natural language processing (NLP).', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you explain it in simple words?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is like a giant collection of texts in over 100 different languages. It's like a library, but instead of books, it has sentences and paragraphs in different languages. Researchers use LangChain to study how computers can understand and translate languages.\", additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Explore Gemini Features\n",
        "Fine-tune Gemini responses by adjusting parameters like temperature and max_tokens.\n",
        "\n",
        "Temperature: Controls creativity (higher = more creative).\n",
        "Max Tokens: Limits response length."
      ],
      "metadata": {
        "id": "BcWu0QItDjg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model settings\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VQNcbf3IDopG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Combining It All**\n",
        "Combine multiple enhancements—like memory, new prompts, and tools—into a full pipeline"
      ],
      "metadata": {
        "id": "izlHtj9GDaV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# configure model\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "# Template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)\n",
        "\n"
      ],
      "metadata": {
        "id": "gG0jxZh0DQAq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run interactions\n",
        "response1 = conversation.run({\"question\": \"define science?\"})\n",
        "print(response1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Y6qhHoJHELhB",
        "outputId": "0615270f-af0e-4e14-d890-33e62da0fb89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Science is a systematic and organized body of knowledge about the natural world, obtained through observation, experimentation, and hypothesis testing. It is based on the belief that the natural world is governed by laws and principles that can be discovered through scientific investigation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"Explain it in a funny way.\"})\n",
        "print(response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "hoUNE77EEiR1",
        "outputId": "6245a87a-bdeb-4f3f-edea-704fc34e7925"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine this: Your brain is like a super-smart butler named \"Walter.\" Walter is in charge of keeping your thoughts organized, but sometimes he's so busy with other tasks (like planning your epic dance moves) that he forgets to do the dishes.\n",
            "\n",
            "When that happens, your thoughts get all messy and tangled up like a plate of spaghetti. That's what absent-mindedness is! It's like Walter forgot to clear the table and now there's a big, tangled mess of thoughts that you can't seem to sort out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response3 = conversation.run({\"question\": \"how many branches of science?\"})\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "b2qpFqJ_EkFf",
        "outputId": "b5994766-8da3-4f64-de81-46913e5fa31d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are **three** main branches of science:\n",
            "\n",
            "1. **Natural science** studies the natural world, including physics, chemistry, biology, and astronomy.\n",
            "\n",
            "2. **Social science** studies human behavior and society, including economics, sociology, psychology, and political science.\n",
            "\n",
            "3. **Formal science** studies abstract concepts, such as mathematics, logic, and computer science.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fts2VarEomh",
        "outputId": "bf5f40e9-16c4-4e6b-e9df-a49f80e152c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='define science?', additional_kwargs={}, response_metadata={}), AIMessage(content='**Science** is the systematic and organized study of the natural world through observation, testing, and experimentation. It seeks to describe and explain natural phenomena, and to make predictions about the future based on evidence. The goal of science is to gain a better understanding of the universe and our place in it.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain it in a funny way.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Imagine your brain is a wacky rollercoaster, with thoughts zooming around like out-of-control minecarts. When you're multitasking, it's like trying to balance on top of all those minecarts while blindfolded and juggling bowling balls. It's a circus that would make even the clowns dizzy!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how many branches of science?', additional_kwargs={}, response_metadata={}), AIMessage(content='There are numerous branches of science, but some major ones include:\\n\\n**Natural Sciences:**\\n\\n* **Physical Sciences:** Physics, Chemistry, Astronomy, Earth Science\\n* **Life Sciences:** Biology, Zoology, Botany, Ecology\\n* **Formal Sciences:** Mathematics, Logic, Computer Science\\n\\n**Social Sciences:**\\n\\n* Sociology\\n* Psychology\\n* Anthropology\\n* Economics\\n* Political Science\\n* Linguistics\\n\\n**Applied Sciences:**\\n\\n* Engineering\\n* Medicine\\n* Agriculture\\n* Computer Science\\n* Materials Science\\n\\n**Interdisciplinary Sciences:**\\n\\n* Environmental Science\\n* Cognitive Science\\n* Neuroscience\\n* Bioinformatics\\n* Data Science\\n\\nThe exact number of branches of science is subjective and can vary depending on classification methods. However, the above list represents the major and commonly recognized fields within the scientific discipline.', additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "Y68ASmmEEv02"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDzz4sGJEyDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}