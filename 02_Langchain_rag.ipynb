{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX2sujD6Kky2"
   },
   "source": [
    "# Project 2: LangChain RAG Project\n",
    "### In this Project, we will create a simple LangChain RAG Colab Notebook that uses the Google Gemini Flash model to answer user questions from the document provided. This example below is provided to help you get started assumes you have access to the Gemini API, Pinecone and a basic Python environment. However, we are required to develop and submit our project using Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW_9njuVKun-"
   },
   "source": [
    "## Step 1 : Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_KO1qUg2KWC1"
   },
   "outputs": [],
   "source": [
    "!pip install langchain pinecone-client google-generativeai openai tqdm python-dotenv -q\n",
    "!pip install langchain-google-genai -U -q\n",
    "!pip install --upgrade langchain -q\n",
    "!pip install langchain-community -U -q\n",
    "!pip install -qU langchain-google-vertexai\n",
    "!pip install google-generativeai --upgrade -q\n",
    "!pip install langchain-google-vertexai --upgrade -q\n",
    "!pip install google-cloud google-cloud-language google-auth google-auth-oauthlib google-auth-httplib2 -q\n",
    "!pip install -U langchain-google-vertexai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekrRdXDrK0rl"
   },
   "source": [
    "### Steps to Create a .env File in Colab\n",
    "Write the .env File to the Current Directory Use the following code snippet to create and save a .env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRfylE8qKlu4",
    "outputId": "480d59f2-ebfe-4aa7-bbd4-2c9a06459e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# Write API keys to a .env file\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(\"PINECONE_API_KEY=\")\n",
    "    f.write(\"PINECONE_ENVIRONMENT=us-east-1'\\n\")\n",
    "    f.write(\"GOOGLE_API_KEY=\\n\")\n",
    "print(\".env file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9MRWv5kK8W-"
   },
   "source": [
    "## Step 2 : Set Up Environment Variables\n",
    "\n",
    "- Create API key variables for secure access.\n",
    "- Use the python-dotenv package to manage the API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "v44CsjNLK7xm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "GOOGLE_GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IYu467GK-zW",
    "outputId": "13a120a6-8091-4fb3-948f-cd91b4f82294"
   },
   "outputs": [],
   "source": [
    "# Verify keys\n",
    "#print(\"Pinecone API Key:\", os.environ[\"PINECONE_API_KEY\"])\n",
    "#print(\"Google Gemini API Key:\", os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVPtrseoLFJl"
   },
   "source": [
    "## Step 3 : Initialize Pinecone\n",
    "pinecone is used to store and retrieve vectorized documents. at first we have to create the index , but running the notebook again can cause errors due to existing index that is why we can use excisting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tRrgAoElLA5K"
   },
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "#from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "#pc = Pinecone(api_key=PINECONE_API_KEY, environment = PINECONE_ENVIRONMENT)\n",
    "\n",
    "# Create a new index or connect to an existing one\n",
    "#index_name = \"gemini-rag-index\"\n",
    "#if index_name not in pc.list_indexes():\n",
    "    # Define the index specification using ServerlessSpec\n",
    "    #spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    #pc.create_index(index_name, dimension=768, spec=spec) # Provide the spec argument\n",
    "\n",
    "# Connect to the index\n",
    "#index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfGiF2piLUUt",
    "outputId": "c62b3ac2-28e9-4e02-a97e-bf651f4a2eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexes': [{'deletion_protection': 'disabled',\n",
      "              'dimension': 768,\n",
      "              'host': 'gemini-rag-index-ppwml7v.svc.aped-4627-b74a.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'gemini-rag-index',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]}\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Instead of using pinecone.init(), create a Pinecone instance:\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "\n",
    "index_name = \"gemini-rag-index\"\n",
    "# Call list_indexes() on the Pinecone instance 'pc'\n",
    "if index_name not in pc.list_indexes().names():  # Use pc.list_indexes().names() to get the index names\n",
    "    pinecone.create_index(index_name, dimension=768)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "# Now you can use 'pc' to interact with Pinecone:\n",
    "print(pc.list_indexes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVO-dvLcMH3T"
   },
   "source": [
    "## Step 4 : Use LangChain for RAG Workflow\n",
    "1. Set Up Embedding Model\n",
    "Use Google Gemini embeddings to vectorize documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XNgXYdpLI-z",
    "outputId": "5ffec746-ea2c-465a-bba2-6edadc726a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 253 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the documents, specifying the correct encoding\n",
    "loader = TextLoader(\"MACHINE LEARNING MODULE 3.txt\", encoding='latin-1')  # Try 'latin-1' or 'cp1252'\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI0LvPkzMQ0N"
   },
   "source": [
    "## Step 5 : Embed and Store Documents in Pinecone\n",
    "Use Google Gemini embeddings to vectorize the document chunks and upload to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QE6QMAqwOrVs",
    "outputId": "dd2062a8-c24c-4f79-ebfd-f671865043dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [10:14<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors upserted into Pinecone.\n",
      "Documents uploaded to Pinecone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=GOOGLE_GEMINI_API_KEY)\n",
    "\n",
    "# Upsert vectors into Pinecone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Upsert vectors into Pinecone\n",
    "for i, doc in enumerate(tqdm(docs)):\n",
    "    vector = embeddings.embed_query(doc.page_content)\n",
    "    index.upsert([(str(i), vector, {\"text\": doc.page_content})])\n",
    "    time.sleep(2)  # Introduce a 1-second delay between requests\n",
    "print(\"Vectors upserted into Pinecone.\")\n",
    "\n",
    "print(\"Documents uploaded to Pinecone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2oiXeera9o7"
   },
   "source": [
    "## Step 7: Set Up Retriever\n",
    "Connect LangChain’s Pinecone Retriever to fetch relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrlrUjojbsqP",
    "outputId": "e91c0d55-6eb2-468b-f50f-7270c1b90ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever setup complete.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Use from_documents to create the Pinecone retriever, and specify the index_name\n",
    "retriever = Pinecone.from_documents(\n",
    "    docs, embeddings, index_name=\"gemini-rag-index\", text_key=\"text\"\n",
    ")\n",
    "print(\"Retriever setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6fdjkAkbOHe"
   },
   "source": [
    "## Step 8: Initialize Google Gemini Flash Model\n",
    "Use Google Gemini Flash as the LLM for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6zjAk_icuFA",
    "outputId": "c7e51e65-bc48-4dd6-82b8-74117b7640bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini Flash model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Set up Google Gemini Flash model\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "\n",
    "# Access the API key using os.environ\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Specify the model name, e.g., 'gemini-1.5-flash'\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"], #Use the environment variable for the API key\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Google Gemini Flash model initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn3DSBvVczPg"
   },
   "source": [
    "## Step 9: Combine Retriever and LLM with LangChain\n",
    "Use the RetrievalQA chain to combine document retrieval with the Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o29pg1CsdB6C",
    "outputId": "25c4ee82-2520-4f93-9e65-3ad364811e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores.pinecone import Pinecone\n",
    "from langchain.chains.retrieval_qa.base import BaseRetrievalQA\n",
    "\n",
    "# Setup RetrievalQA chain\n",
    "# Instead of directly using Pinecone, use Pinecone.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever.as_retriever()  # Use as_retriever() to get a BaseRetriever\n",
    ")\n",
    "print(\"RetrievalQA chain ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukmwId-fdIHm"
   },
   "source": [
    "## Step 10: Query the RAG System\n",
    "Now, you can test your RAG system with questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UV9k65pb_r8",
    "outputId": "aca5c45e-7d5d-45ea-85b2-031db863bced"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-b516ac1321d9>:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning?\n",
      "Response: Machine learning (ML) is a discipline of artificial intelligence (AI) that provides machines with the ability to automatically learn from data and past experiences while identifying patterns to make predictions with minimal human intervention.  It's a method of training a machine to learn from data, rather than being explicitly programmed.  This involves feeding a machine a large dataset and allowing it to learn and improve performance over time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is machine learning?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzBvA5ajb8zP",
    "outputId": "c23264b0-7793-42fe-8392-964eb2df9796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the types of machine learning?\n",
      "Response: Machine learning algorithms can be broadly categorized into supervised, unsupervised, semi-supervised, and reinforcement learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the types of machine learning?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qo1TcYR7gMjB",
    "outputId": "3bea122a-c009-4e35-8e48-da52ec89c0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: explain types of machine learning?\n",
      "Response: Machine learning algorithms can be broadly categorized into supervised, unsupervised, semi-supervised, and reinforcement learning.  Each category has specific types of algorithms designed for particular kinds of tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"explain types of machine learning?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOfyQu6aV6te"
   },
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FLuyh75hQT4"
   },
   "source": [
    "## Step 7: Deploy and Iterate\n",
    "Deploy the RAG system as an API using FastAPI or Flask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v3DjqDpCxPr"
   },
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaU9CZ-YlboL"
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiXBYQP-ONZG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
